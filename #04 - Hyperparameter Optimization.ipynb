{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Recap\n",
    "==\n",
    "\n",
    "\n",
    "In the last mission, we focused on increasing the number of attributes the model uses. We saw how, in general, adding more attributes generally lowered the error of the model. This is because the model is able to do a better job identifying the living spaces from the training set that are the most similar to the ones from the test set. \n",
    "\n",
    "In this mission, we'll focus on the impact of increasing **k**, the number of nearby neighbors the model uses to make predictions. \n",
    "\n",
    "For now, we do not need to re-invent the wheell and lets use our \"Swiss Army Knife\" to prepare the data.\n",
    "\n",
    "Exercise Start.\n",
    "==\n",
    "\n",
    "\n",
    "**Description**: \n",
    "\n",
    "1. Read **dc_airbnb.csv** into a Dataframe and assign to **dc_listings**.\n",
    "2. Prepare and cleaning **dc_listings** according the previous mission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dc_listings = pd.read_csv('dc_airbnb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cleaning(df):\n",
    "    \n",
    "    # format the date\n",
    "    stripped_commas = df['price'].str.replace(',', '')\n",
    "    stripped_dollars = stripped_commas.str.replace('$', '')\n",
    "    df['price'] = stripped_dollars.astype('float')\n",
    "\n",
    "    # drop the unless columns\n",
    "    columns = [\"room_type\", \"city\", \"state\",\"latitude\",\"longitude\",\"zipcode\",\n",
    "          \"host_response_rate\",\"host_acceptance_rate\",\"host_listings_count\",\n",
    "          \"cleaning_fee\", \"security_deposit\"]\n",
    "    df.drop(columns,axis=1,inplace=True)\n",
    "    \n",
    "    # drop null rows\n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "dc_listings = prepare_cleaning(dc_listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Standardization\n",
    "==\n",
    "\n",
    "[Standardization](http://scikit-learn.org/stable/modules/preprocessing.html) of datasets is a common requirement for many machine learning estimators implemented in **scikit-learn**; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# store the target column\n",
    "target = dc_listings.price\n",
    "\n",
    "# scale the features\n",
    "scaled_features = StandardScaler().fit_transform(dc_listings.values)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, \n",
    "                                  index=dc_listings.index, \n",
    "                                  columns=dc_listings.columns)\n",
    "\n",
    "# update the original price column\n",
    "scaled_features_df.price = target\n",
    "\n",
    "# show the five first rows\n",
    "scaled_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Splitting Our Trainning Data\n",
    "==\n",
    "\n",
    "The scikit-learn library has a [handy model_selection.train_test_split()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function that we can use to split our data. **train_test_split()** accepts two parameters, **X** and **y**, which contain all the data we want to train and test on, and returns four objects: **train_X**, **train_y**, **test_X**, **test_y**:\n",
    "\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=12lermQWApWq5kmvM0HOYW0eUzIqCIQ8u\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = [\"accommodates\", \"bedrooms\", \"bathrooms\", \"beds\",\n",
    "           \"minimum_nights\", \"maximum_nights\", \"number_of_reviews\"]\n",
    "\n",
    "X = dc_listings[features]\n",
    "y = dc_listings['price']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.20,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Hyperparameter optimization\n",
    "==\n",
    "\n",
    "When we vary the features that are used in the model, we're affecting the data that the model uses. On the other hand, varying the **k** value affects the behavior of the model independently of the actual data that's used when making predictions. In other words, we're impacting how the model performs without trying to change the data that's used.\n",
    "\n",
    "Values that affect the behavior and performance of a model that are unrelated to the data that's used are referred to as **hyperparameters**. The process of finding the optimal hyperparameter value is known as hyperparameter optimization. A simple but common [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) technique is known as [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search), which involves:\n",
    "\n",
    "- selecting a subset of the possible hyperparameter values,\n",
    "- training a model using each of these hyperparameter values,\n",
    "- evaluating each model's performance,\n",
    "- selecting the hyperparameter value that resulted in the lowest error value.\n",
    "\n",
    "Grid search essentially boils down to evaluating the model performance at different k values and selecting the k value that resulted in the lowest error. While grid search can take a long time when working with large datasets, the data we're working with in this mission is small and this process is relatively quick.\n",
    "\n",
    "Let's confirm that grid search will work quickly for the dataset we're working with by first observing how the model performance changes as we increase the k value from **1** to **5**. If you recall, we set **5** as the **k** value for the last 2 missions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Holdout Validation\n",
    "\n",
    "\n",
    "In an earlier mission, we learned about train/test validation, a simple technique for testing a machine learning model's accuracy on new data that the model wasn't trained on. In this mission, we'll focus on more robust techniques.\n",
    "\n",
    "To start, we'll focus on the **holdout validation** technique, which involves:\n",
    "\n",
    "- splitting the full dataset into 2 partitions:\n",
    "    - a training set\n",
    "    - a test set\n",
    "- training the model on the training set,\n",
    "- using the trained model to predict labels on the test set,\n",
    "- computing an error metric to understand the model's effectiveness,\n",
    "- switch the training and test sets and repeat,\n",
    "- average the errors.\n",
    "\n",
    "In holdout validation, we usually use a 50/50 split instead of the 75/25 split from train/test validation. This way, we remove number of observations as a potential source of variation in our model performance.\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Nwq2puuGhziyQ82eukrPctHQ8UGJ93Vt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. K-Fold Cross Validation\n",
    "\n",
    "\n",
    "**Holdout** validation is actually a specific example of a larger class of validation techniques called **k-fold cross-validation**. While holdout validation is better than train/test validation because the model isn't repeatedly biased towards a specific subset of the data, both models that are trained only use half the available data. K-fold cross validation, on the other hand, takes advantage of a larger proportion of the data during training while still rotating through different subsets of the data to avoid the issues of train/test validation.\n",
    "\n",
    "Here's the algorithm from k-fold cross validation:\n",
    "\n",
    "- splitting the full dataset into **k** equal length partitions,\n",
    "    - selecting **k-1** partitions as the training set and\n",
    "    - selecting the remaining partition as the test set\n",
    "- training the model on the training set,\n",
    "- using the trained model to predict labels on the test fold,\n",
    "- computing the test fold's error metric,\n",
    "- repeating all of the above steps **k-1** times, until each partition has been used as the test set for an iteration,\n",
    "- calculating the mean of the **k** error values.\n",
    "\n",
    "Holdout validation is essentially a version of k-fold cross validation when **k** is equal to **2**. Generally, **5** or **10** folds is used for k-fold cross-validation. Here's a diagram describing each iteration of 5-fold cross validation:\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1i9sScUbJqDLeCtd3InrBbaeHXI7vkAyQ\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# configure the hyperparameters\n",
    "hyperparameters = {\n",
    "    \"n_neighbors\": range(1,21,1)\n",
    "}\n",
    "\n",
    "# instantiate a KNN model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# execute a gridsearch procedure\n",
    "grid = GridSearchCV(knn,\n",
    "                    param_grid=hyperparameters,\n",
    "                    cv=10,\n",
    "                    scoring=\"neg_mean_squared_error\",\n",
    "                    return_train_score=True)\n",
    "\n",
    "# fit using train data\n",
    "grid.fit(train_X, train_y)\n",
    "\n",
    "# look the best results\n",
    "best_params = grid.best_params_\n",
    "best_score = grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\\nBest score (mse): {}\".format(best_params,np.absolute(best_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid.cv_results_['mean_test_score']\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_std = grid.cv_results_['std_test_score']\n",
    "results_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "ax1.plot(range(1,21),np.absolute(results))\n",
    "ax1.set_xlabel(\"K neighbors\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.set_xticks(range(1,21))\n",
    "\n",
    "ax2.plot(range(1,21),results_std)\n",
    "ax2.set_xlabel(\"K neighbors\")\n",
    "ax2.set_ylabel(\"std MSE\")\n",
    "ax2.set_xticks(range(1,21))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise Start\n",
    "==\n",
    "\n",
    "**Description**\n",
    "\n",
    "1. Study the [KNeighborRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) API and test new parameters in **hyperparameters** variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Bias-Variance Tradeoff\n",
    "\n",
    "So far, we've been working under the assumption that a lower RMSE always means that a model is more accurate. This isn't the complete picture, unfortunately. A model has two sources of error, **bias** and **variance**.\n",
    "\n",
    "Bias describes error that results in bad assumptions about the learning algorithm. For example, assuming that only one feature, like a car's weight, relates to a car's fuel efficiency will lead you to fit a simple, univariate regression model that will result in high bias. The error rate will be high since a car's fuel efficiency is affected by many other factors besides just its weight.\n",
    "\n",
    "Variance describes error that occurs because of the variability of a model's predicted values. If we were given a dataset with 1000 features on each car and used every single feature to train an incredibly complicated multivariate regression model, we will have low bias but high variance. In an ideal world, we want low bias and low variance but in reality, there's always a tradeoff.\n",
    "\n",
    "The standard deviation of the RMSE values can be a proxy for a model's **variance** while the average RMSE is a proxy for a model's **bias**. Bias and variance are the 2 observable sources of error in a model that we can indirectly control.\n",
    "\n",
    "\n",
    "<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1Ws58dUyjBEmDPzqZA6vRmShScFZxoeEV\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Make predictions on unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# best model\n",
    "best_knn = grid.best_estimator_\n",
    "\n",
    "# predict using the best model with unseen X\n",
    "predictions = best_knn.predict(test_X)\n",
    "\n",
    "mse = mean_squared_error(test_y,predictions)\n",
    "\n",
    "print(\"mse: {}\\nrmse: {}\".format(mse,np.sqrt(mse)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
